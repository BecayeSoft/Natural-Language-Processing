{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentence Segmentation\n",
    "spaCy has its own rules to divide sentences. However, there are times we might need our own."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right; leadership is doing the right things.\" |\n",
      "-Peter Drucker |\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(f'{sent} |')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add a segmentation rule"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \"\n",
      "1: Management\n",
      "2: is\n",
      "3: doing\n",
      "4: things\n",
      "5: right\n",
      "6: ;\n",
      "7: leadership\n",
      "8: is\n",
      "9: doing\n",
      "10: the\n",
      "11: right\n",
      "12: things\n",
      "13: .\n",
      "14: \"\n",
      "15: -Peter\n"
     ]
    }
   ],
   "source": [
    "# token.i returns the index of the token. We will use it to define the start of the sentences.\n",
    "for token in doc[:-1]:\n",
    "    print(f'{token.i}: {token}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the rule\n",
    "Our rule will be the following: Divide the sentences by semicolon (;)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    for token in doc:\n",
    "        if token.text == ';':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding the rule to the pipeline\n",
    "We convert the function into a spaCy component then add it to the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['tok2vec',\n 'tagger',\n 'custom_boundaries',\n 'parser',\n 'attribute_ruler',\n 'lemmatizer',\n 'ner']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "@Language.component('custom_boundaries')\n",
    "def custom_boundaries_component(doc):\n",
    "    return set_custom_boundaries(doc)\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe('custom_boundaries', before='parser')\n",
    "nlp.pipe_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing\n",
    "This time, the text is split into three sentences instead of two."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Management is doing things right;\n",
      "leadership is doing the right things.\"\n",
      "-Peter Drucker\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
    "\n",
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Changing spaCy's segmentation rules\n",
    "Imagine we are writing a poem. We don't want to split the text by \".\" but rather line breaks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')  # reset pipeline\n",
    "\n",
    "poem = '''Whispers in twilight's embrace,\n",
    "Silent dreams, a celestial chase.\n",
    "Stars dance, painting night's grace. Glimmers of hope guide our way.'''\n",
    "\n",
    "poem_doc = nlp(poem)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Whispers in twilight's embrace,\n",
      "Silent dreams, a celestial chase.\n",
      "\n",
      "2. Stars dance, painting night's grace.\n",
      "3. Glimmers of hope guide our way.\n"
     ]
    }
   ],
   "source": [
    "poem_doc = nlp(poem_doc)\n",
    "\n",
    "for i, sent in enumerate(poem_doc.sents):\n",
    "    print(f'{i+1}. {sent}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a rule\n",
    "Our rule will be the following: split sentences by line.\n",
    "\n",
    "**Note**: `yield` is used for memory-efficiency which is useful when working with large variable such as large documents for example.\n",
    "`yield` generates the values of a generator one by one. It suspends the execution and wait till the next() method is called to resume."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    start = 0\n",
    "    new_line = False\n",
    "\n",
    "    for word in doc:\n",
    "        if new_line:\n",
    "            doc[start:word.i].is_sent_start = True\n",
    "            new_line = False\n",
    "            start = word.i\n",
    "        elif word.text.startswith('\\n'):\n",
    "            new_line = True\n",
    "\n",
    "    doc[start:].is_sent_start = True\n",
    "\n",
    "    return doc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'is_sent_start'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m nlp\u001B[38;5;241m.\u001B[39madd_pipe(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset_custom_boundaries\u001B[39m\u001B[38;5;124m\"\u001B[39m, before\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m poem_doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m([sent\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msents])\n",
      "File \u001B[1;32mD:\\Natural Language Processing\\Notebooks-env\\Lib\\site-packages\\spacy\\language.py:1016\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1014\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1016\u001B[0m     \u001B[43merror_handler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(doc, Doc):\n\u001B[0;32m   1018\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE005\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, returned_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mtype\u001B[39m(doc)))\n",
      "File \u001B[1;32mD:\\Natural Language Processing\\Notebooks-env\\Lib\\site-packages\\spacy\\util.py:1689\u001B[0m, in \u001B[0;36mraise_error\u001B[1;34m(proc_name, proc, docs, e)\u001B[0m\n\u001B[0;32m   1688\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_error\u001B[39m(proc_name, proc, docs, e):\n\u001B[1;32m-> 1689\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[1;32mD:\\Natural Language Processing\\Notebooks-env\\Lib\\site-packages\\spacy\\language.py:1011\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1009\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1010\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1011\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1012\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1013\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1014\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[14], line 8\u001B[0m, in \u001B[0;36mset_custom_boundaries\u001B[1;34m(doc)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m doc:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m new_line:\n\u001B[1;32m----> 8\u001B[0m         \u001B[43mdoc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m:\u001B[49m\u001B[43mword\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_sent_start\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m      9\u001B[0m         new_line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     10\u001B[0m         start \u001B[38;5;241m=\u001B[39m word\u001B[38;5;241m.\u001B[39mi\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'spacy.tokens.span.Span' object has no attribute 'is_sent_start'"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "poem_doc = nlp(poem)\n",
    "\n",
    "print([sent.text for sent in doc.sents])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_on_newlines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tok \u001B[38;5;129;01min\u001B[39;00m \u001B[43msplit_on_newlines\u001B[49m(doc):\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28mprint\u001B[39m(tok)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'split_on_newlines' is not defined"
     ]
    }
   ],
   "source": [
    "for tok in split_on_newlines(doc):\n",
    "    print(tok)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
