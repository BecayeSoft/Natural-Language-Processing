{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word2Vec\n",
    "Word2Vec is a technique for embedding words in a vector of numbers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-2.63023090e+00, -1.44064039e-01, -2.87560010e+00, -9.58000004e-01,\n        6.34711599e+00,  1.81727195e+00, -2.13021231e+00,  3.74393988e+00,\n       -4.01097953e-01, -8.92248034e-01,  7.67081976e+00,  2.34689999e+00,\n       -5.68435192e+00,  1.63542008e+00,  4.29126406e+00,  2.65903997e+00,\n       -5.76235414e-01, -9.24070001e-01, -1.19463965e-01, -6.29390049e+00,\n        1.71580088e+00, -1.94941783e+00, -2.66502976e+00, -2.97551990e+00,\n       -1.85918009e+00,  1.46602595e+00, -2.17178011e+00, -1.36144006e+00,\n       -8.78052115e-01,  1.18097997e+00,  3.28314829e+00, -7.96960056e-01,\n       -4.79652023e+00, -7.15170002e+00, -2.68129015e+00,  9.29199904e-02,\n       -7.71052003e-01,  3.14497995e+00,  3.59101987e+00,  2.20619392e+00,\n        2.29066801e+00,  3.84905577e+00,  1.83126199e+00,  6.30881339e-02,\n       -3.43605995e+00,  2.61152601e+00,  1.49480021e+00, -3.63001895e+00,\n       -3.32800001e-01,  2.76520014e-01, -6.39757991e-01,  4.56192017e+00,\n        1.36009812e+00, -7.86274052e+00,  9.99456048e-01,  9.04163957e-01,\n       -2.39053798e+00,  1.58872390e+00,  1.68631959e+00, -2.17739910e-01,\n        2.16494417e+00, -1.72320008e+00,  7.36760348e-02, -8.71508777e-01,\n        2.74817991e+00, -7.07419664e-02, -4.69236040e+00, -3.54220200e+00,\n        1.69379994e-01,  1.88213789e+00,  8.70003738e-03, -1.04577386e+00,\n       -2.26790190e+00, -1.79282188e+00,  2.02602386e+00,  1.67140011e-02,\n       -3.77238011e+00,  5.43949962e-01, -1.50208199e+00,  3.46120983e-01,\n       -4.04522228e+00, -3.15467387e-01, -1.64388943e+00,  1.05197966e+00,\n        3.57464027e+00, -1.40522408e+00, -2.78792048e+00, -1.94138789e+00,\n        4.32094097e-01,  1.58418798e+00, -7.77289987e-01,  2.58469969e-01,\n       -2.77457595e-01, -4.94143963e+00, -7.76661992e-01, -1.56867206e+00,\n        1.26172805e+00, -1.98749995e+00,  1.33273995e+00,  7.49788165e-01,\n        1.75266612e+00,  1.10650671e+00,  3.17264199e+00,  2.69628787e+00,\n        3.10640007e-01,  3.88644409e+00, -3.75656009e+00, -1.66012019e-01,\n       -2.29589605e+00,  8.43403995e-01,  2.92140007e+00, -5.31838238e-01,\n       -9.82728004e-01, -4.20054048e-01, -2.75039971e-01,  4.85932016e+00,\n       -9.68058109e-01, -9.64512050e-01,  4.28733975e-01, -1.48210597e+00,\n       -2.75171995e+00,  6.62779987e-01, -1.03262818e+00,  3.22439998e-01,\n       -2.54419994e+00, -5.47237015e+00,  1.24408793e+00, -4.61620998e+00,\n        1.28907537e+00,  1.14482403e+00, -4.21149826e+00, -2.30453992e+00,\n        2.66967010e+00,  2.31400486e-02, -8.09979588e-02,  3.76224399e+00,\n       -3.23614001e+00, -1.04071999e+00,  2.53508401e+00, -2.01156187e+00,\n       -1.73155785e+00, -2.68436003e+00, -1.25398004e+00, -6.16979957e-01,\n        1.63680398e+00,  2.40040775e-02, -3.93810010e+00, -2.82359791e+00,\n        1.69786453e-01,  2.34513998e-01,  2.58140016e+00,  2.29427981e+00,\n       -3.59941959e-01,  2.69364059e-01,  3.90059948e-02,  8.92249942e-01,\n        5.69349194e+00,  2.54483998e-01, -1.06792605e+00, -1.05397725e+00,\n       -1.40341604e+00, -3.53493810e+00, -5.60948014e-01,  4.76564020e-01,\n       -3.24745989e+00, -1.22757995e+00, -4.71065998e+00,  7.34767556e-01,\n        1.98719978e-01, -1.99819997e-01,  1.40298009e+00, -1.97121203e+00,\n        2.11287808e+00, -3.60490024e-01,  1.64822006e+00, -7.71001935e-01,\n        1.28310055e-01,  2.62718201e+00, -1.14745998e+00, -5.01203954e-01,\n       -3.12197590e+00, -1.98807597e+00,  5.00232995e-01, -1.27377403e+00,\n       -1.32284200e+00,  4.62472051e-01, -3.65417361e+00, -2.67410588e+00,\n       -8.75981175e-04,  3.59770823e+00, -1.73322606e+00,  4.79932010e-01,\n        2.02968001e+00, -1.77698195e+00,  4.67644066e-01,  7.96051979e-01,\n       -3.49050975e+00,  2.03348017e+00,  7.59376049e-01, -7.66705990e-01,\n       -1.18902013e-01, -3.05726600e+00, -7.24695981e-01, -2.31209207e+00,\n        3.37934375e+00,  4.03675973e-01, -3.64507985e+00,  2.46456003e+00,\n       -3.15800197e-02,  1.80601406e+00,  2.50769377e+00,  2.14339972e+00,\n       -2.12896419e+00, -1.27473998e+00, -1.74891186e+00,  3.23573351e+00,\n       -1.14230800e+00, -4.41306973e+00, -1.72609389e+00,  5.44142008e-01,\n       -1.68166006e+00,  1.99374199e+00,  8.81866097e-01,  1.72128010e+00,\n       -2.17501712e+00,  9.46338058e-01, -5.28939068e-01,  2.85106015e+00,\n        4.21815968e+00,  1.60893989e+00,  5.20119965e-01, -2.34509993e+00,\n        7.42384017e-01,  3.55149794e+00,  1.73868787e+00,  2.18861008e+00,\n       -9.31398034e-01,  1.52694201e+00, -9.19333816e-01, -1.42623976e-01,\n       -2.02883983e+00, -7.98676014e-01,  2.11800003e+00, -1.08550799e+00,\n       -2.54154015e+00,  1.59976006e+00, -4.31851673e+00,  2.94197977e-01,\n        1.47531414e+00,  3.64882421e+00, -1.23579192e+00, -1.97214603e+00,\n       -5.04694033e+00, -3.89691815e-02, -9.54017162e-01, -2.86428499e+00,\n        3.41750002e+00,  1.05794847e+00, -1.43697989e+00,  1.96797812e+00,\n        7.14439988e-01,  4.43962002e+00,  2.16688776e+00,  1.80559599e+00,\n        3.20032620e+00, -1.65139997e+00, -4.71975982e-01,  3.39680028e+00,\n       -4.46087980e+00,  8.46774101e-01,  4.19156981e+00, -2.89289808e+00,\n        5.86840034e-01, -1.67589605e+00,  1.21833014e+00, -1.35881388e+00,\n       -1.55852413e+00, -1.14900410e+00, -1.73038387e+00,  2.34208393e+00,\n        2.89951181e+00,  1.58689594e+00, -9.33580041e-01, -8.14199924e-01,\n       -8.03188026e-01, -1.25111604e+00, -2.64727026e-01,  2.04702401e+00,\n       -2.03597784e+00, -3.89297992e-01,  1.00610590e+00, -2.61201978e+00,\n        2.62038851e+00, -1.54340267e-02, -2.31673980e+00, -1.62956011e+00,\n       -2.97676015e+00, -1.46563399e+00, -3.81239820e+00,  1.42264402e+00],\n      dtype=float32)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('dogs are the best.')\n",
    "print(f'Shape: {doc.vector.shape}')\n",
    "doc.vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([-2.63023090e+00, -1.44064039e-01, -2.87560010e+00, -9.58000004e-01,\n        6.34711599e+00,  1.81727195e+00, -2.13021231e+00,  3.74393988e+00,\n       -4.01097953e-01, -8.92248034e-01,  7.67081976e+00,  2.34689999e+00,\n       -5.68435192e+00,  1.63542008e+00,  4.29126406e+00,  2.65903997e+00,\n       -5.76235414e-01, -9.24070001e-01, -1.19463965e-01, -6.29390049e+00,\n        1.71580088e+00, -1.94941783e+00, -2.66502976e+00, -2.97551990e+00,\n       -1.85918009e+00,  1.46602595e+00, -2.17178011e+00, -1.36144006e+00,\n       -8.78052115e-01,  1.18097997e+00,  3.28314829e+00, -7.96960056e-01,\n       -4.79652023e+00, -7.15170002e+00, -2.68129015e+00,  9.29199904e-02,\n       -7.71052003e-01,  3.14497995e+00,  3.59101987e+00,  2.20619392e+00,\n        2.29066801e+00,  3.84905577e+00,  1.83126199e+00,  6.30881339e-02,\n       -3.43605995e+00,  2.61152601e+00,  1.49480021e+00, -3.63001895e+00,\n       -3.32800001e-01,  2.76520014e-01, -6.39757991e-01,  4.56192017e+00,\n        1.36009812e+00, -7.86274052e+00,  9.99456048e-01,  9.04163957e-01,\n       -2.39053798e+00,  1.58872390e+00,  1.68631959e+00, -2.17739910e-01,\n        2.16494417e+00, -1.72320008e+00,  7.36760348e-02, -8.71508777e-01,\n        2.74817991e+00, -7.07419664e-02, -4.69236040e+00, -3.54220200e+00,\n        1.69379994e-01,  1.88213789e+00,  8.70003738e-03, -1.04577386e+00,\n       -2.26790190e+00, -1.79282188e+00,  2.02602386e+00,  1.67140011e-02,\n       -3.77238011e+00,  5.43949962e-01, -1.50208199e+00,  3.46120983e-01,\n       -4.04522228e+00, -3.15467387e-01, -1.64388943e+00,  1.05197966e+00,\n        3.57464027e+00, -1.40522408e+00, -2.78792048e+00, -1.94138789e+00,\n        4.32094097e-01,  1.58418798e+00, -7.77289987e-01,  2.58469969e-01,\n       -2.77457595e-01, -4.94143963e+00, -7.76661992e-01, -1.56867206e+00,\n        1.26172805e+00, -1.98749995e+00,  1.33273995e+00,  7.49788165e-01,\n        1.75266612e+00,  1.10650671e+00,  3.17264199e+00,  2.69628787e+00,\n        3.10640007e-01,  3.88644409e+00, -3.75656009e+00, -1.66012019e-01,\n       -2.29589605e+00,  8.43403995e-01,  2.92140007e+00, -5.31838238e-01,\n       -9.82728004e-01, -4.20054048e-01, -2.75039971e-01,  4.85932016e+00,\n       -9.68058109e-01, -9.64512050e-01,  4.28733975e-01, -1.48210597e+00,\n       -2.75171995e+00,  6.62779987e-01, -1.03262818e+00,  3.22439998e-01,\n       -2.54419994e+00, -5.47237015e+00,  1.24408793e+00, -4.61620998e+00,\n        1.28907537e+00,  1.14482403e+00, -4.21149826e+00, -2.30453992e+00,\n        2.66967010e+00,  2.31400486e-02, -8.09979588e-02,  3.76224399e+00,\n       -3.23614001e+00, -1.04071999e+00,  2.53508401e+00, -2.01156187e+00,\n       -1.73155785e+00, -2.68436003e+00, -1.25398004e+00, -6.16979957e-01,\n        1.63680398e+00,  2.40040775e-02, -3.93810010e+00, -2.82359791e+00,\n        1.69786453e-01,  2.34513998e-01,  2.58140016e+00,  2.29427981e+00,\n       -3.59941959e-01,  2.69364059e-01,  3.90059948e-02,  8.92249942e-01,\n        5.69349194e+00,  2.54483998e-01, -1.06792605e+00, -1.05397725e+00,\n       -1.40341604e+00, -3.53493810e+00, -5.60948014e-01,  4.76564020e-01,\n       -3.24745989e+00, -1.22757995e+00, -4.71065998e+00,  7.34767556e-01,\n        1.98719978e-01, -1.99819997e-01,  1.40298009e+00, -1.97121203e+00,\n        2.11287808e+00, -3.60490024e-01,  1.64822006e+00, -7.71001935e-01,\n        1.28310055e-01,  2.62718201e+00, -1.14745998e+00, -5.01203954e-01,\n       -3.12197590e+00, -1.98807597e+00,  5.00232995e-01, -1.27377403e+00,\n       -1.32284200e+00,  4.62472051e-01, -3.65417361e+00, -2.67410588e+00,\n       -8.75981175e-04,  3.59770823e+00, -1.73322606e+00,  4.79932010e-01,\n        2.02968001e+00, -1.77698195e+00,  4.67644066e-01,  7.96051979e-01,\n       -3.49050975e+00,  2.03348017e+00,  7.59376049e-01, -7.66705990e-01,\n       -1.18902013e-01, -3.05726600e+00, -7.24695981e-01, -2.31209207e+00,\n        3.37934375e+00,  4.03675973e-01, -3.64507985e+00,  2.46456003e+00,\n       -3.15800197e-02,  1.80601406e+00,  2.50769377e+00,  2.14339972e+00,\n       -2.12896419e+00, -1.27473998e+00, -1.74891186e+00,  3.23573351e+00,\n       -1.14230800e+00, -4.41306973e+00, -1.72609389e+00,  5.44142008e-01,\n       -1.68166006e+00,  1.99374199e+00,  8.81866097e-01,  1.72128010e+00,\n       -2.17501712e+00,  9.46338058e-01, -5.28939068e-01,  2.85106015e+00,\n        4.21815968e+00,  1.60893989e+00,  5.20119965e-01, -2.34509993e+00,\n        7.42384017e-01,  3.55149794e+00,  1.73868787e+00,  2.18861008e+00,\n       -9.31398034e-01,  1.52694201e+00, -9.19333816e-01, -1.42623976e-01,\n       -2.02883983e+00, -7.98676014e-01,  2.11800003e+00, -1.08550799e+00,\n       -2.54154015e+00,  1.59976006e+00, -4.31851673e+00,  2.94197977e-01,\n        1.47531414e+00,  3.64882421e+00, -1.23579192e+00, -1.97214603e+00,\n       -5.04694033e+00, -3.89691815e-02, -9.54017162e-01, -2.86428499e+00,\n        3.41750002e+00,  1.05794847e+00, -1.43697989e+00,  1.96797812e+00,\n        7.14439988e-01,  4.43962002e+00,  2.16688776e+00,  1.80559599e+00,\n        3.20032620e+00, -1.65139997e+00, -4.71975982e-01,  3.39680028e+00,\n       -4.46087980e+00,  8.46774101e-01,  4.19156981e+00, -2.89289808e+00,\n        5.86840034e-01, -1.67589605e+00,  1.21833014e+00, -1.35881388e+00,\n       -1.55852413e+00, -1.14900410e+00, -1.73038387e+00,  2.34208393e+00,\n        2.89951181e+00,  1.58689594e+00, -9.33580041e-01, -8.14199924e-01,\n       -8.03188026e-01, -1.25111604e+00, -2.64727026e-01,  2.04702401e+00,\n       -2.03597784e+00, -3.89297992e-01,  1.00610590e+00, -2.61201978e+00,\n        2.62038851e+00, -1.54340267e-02, -2.31673980e+00, -1.62956011e+00,\n       -2.97676015e+00, -1.46563399e+00, -3.81239820e+00,  1.42264402e+00],\n      dtype=float32)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = nlp('dog')\n",
    "print(f'Shape: {doc.vector.shape}')\n",
    "doc.vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identifying similarities\n",
    "The advantage of embedding words as vectors is that we can perform mathematical operation. Cosine similarity is  one such operation that compute the cosine distance between two words vectors.\n",
    "With 1 being the highest value and 0 the lowest."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def print_similarity(doc):\n",
    "    \"\"\"Print similarities between all the words in the doc.\"\"\"\n",
    "    for token1 in doc:\n",
    "        for token2 in doc:\n",
    "            print(f'{token1.text} - {token2.text}: {token1.similarity(token2)}')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king and queen similarity: 0.6108841628588695\n"
     ]
    }
   ],
   "source": [
    "word1, word2 = nlp('king'), nlp('queen')\n",
    "print('king and queen similarity:', word1.similarity(word2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - king: 1.0\n",
      "king - queen: 0.6108841896057129\n",
      "king - kings: 0.8352225422859192\n",
      "king - princess: 0.6533219218254089\n",
      "\n",
      "queen - king: 0.6108841896057129\n",
      "queen - queen: 1.0\n",
      "queen - kings: 0.510179340839386\n",
      "queen - princess: 0.7273048758506775\n",
      "\n",
      "kings - king: 0.8352225422859192\n",
      "kings - queen: 0.510179340839386\n",
      "kings - kings: 1.0\n",
      "kings - princess: 0.5621244311332703\n",
      "\n",
      "princess - king: 0.6533219218254089\n",
      "princess - queen: 0.7273048758506775\n",
      "princess - kings: 0.5621244311332703\n",
      "princess - princess: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('king queen kings princess')\n",
    "print_similarity(doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display similarites in a table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "<table><tr><th></th><th>king</th><th>queen</th><th>kings</th></tr><tr><td>**king**</td><td>1.0</td><td>0.6109</td><td>0.8352</td></tr><tr><td>**queen**</td><td>0.6109</td><td>1.0</td><td>0.5102</td></tr><tr><td>**kings**</td><td>0.8352</td><td>0.5102</td><td>1.0</td></tr><tr><td>**princess**</td><td>0.6533</td><td>0.7273</td><td>0.5621</td></tr>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For brevity, assign each token a name\n",
    "a,b,c, d = doc\n",
    "\n",
    "# Display as a Markdown table (this only works in Jupyter!)\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f'<table><tr><th></th><th>{a.text}</th><th>{b.text}</th><th>{c.text}</th></tr>\\\n",
    "<tr><td>**{a.text}**</td><td>{a.similarity(a):{.4}}</td><td>{b.similarity(a):{.4}}</td><td>{c.similarity(a):{.4}}</td></tr>\\\n",
    "<tr><td>**{b.text}**</td><td>{a.similarity(b):{.4}}</td><td>{b.similarity(b):{.4}}</td><td>{c.similarity(b):{.4}}</td></tr>\\\n",
    "<tr><td>**{c.text}**</td><td>{a.similarity(c):{.4}}</td><td>{b.similarity(c):{.4}}</td><td>{c.similarity(c):{.4}}</td></tr>\\\n",
    "<tr><td>**{d.text}**</td><td>{a.similarity(d):{.4}}</td><td>{b.similarity(d):{.4}}</td><td>{c.similarity(d):{.4}}</td></tr>'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Similar Opposites\n",
    "Sometimes, words are different in English but very similar in NLP. It's quite fascinating to see how similar 'love' and 'hate'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like - like: 1.0\n",
      "like - love: 0.5212638974189758\n",
      "like - hate: 0.5065141320228577\n",
      "\n",
      "love - like: 0.5212638974189758\n",
      "love - love: 1.0\n",
      "love - hate: 0.5708349943161011\n",
      "\n",
      "hate - like: 0.5065141320228577\n",
      "hate - love: 0.5708349943161011\n",
      "hate - hate: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('like love hate')\n",
    "print_similarity(doc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word outside of vocabulary\n",
    "Words that do not belong to the vocabulary have value 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "dogat False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('dog cat dogat')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vector arithmetic\n",
    "We can calculate new vectors but adding or subtracting related vectors.\n",
    "A famous example is `king - man + woman = queen`.\n",
    "Let's see how we can calculate that."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x, y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1.1296e-01, -4.1865e+00, -1.8453e+00,  3.0781e-01,  2.4956e+00,\n        9.6267e-01, -1.8161e+00,  4.4655e+00, -2.8210e+00,  9.7090e-01,\n        1.3542e+01,  4.3195e-01, -5.3098e+00,  4.7098e+00,  2.9030e+00,\n        1.5588e+00,  6.0064e+00, -3.0345e+00,  1.0626e+00, -7.7197e-01,\n       -5.4771e+00, -9.7380e-01, -4.4345e+00,  5.8367e+00,  2.4302e+00,\n       -3.9408e+00, -9.1862e-01, -4.9124e+00,  1.4591e+00, -7.2772e-01,\n        3.4957e+00, -4.0077e+00, -1.8354e+00, -4.1052e+00,  4.9211e+00,\n       -9.7053e-01,  1.9223e+00,  5.2605e+00,  1.6086e+00,  7.1328e-01,\n       -1.2146e+00, -1.9869e+00,  8.0265e-01,  2.9298e+00,  7.2985e-01,\n       -6.2892e-01, -1.7082e+00,  1.9893e+00,  4.7529e-01,  3.2264e+00,\n       -3.9215e+00,  4.6556e+00,  1.3475e+00, -1.0979e+00, -3.0365e+00,\n        1.5815e+00,  2.2835e+00, -4.0616e+00,  2.5730e+00,  4.0618e+00,\n        9.5438e-01, -6.2563e+00,  5.6463e+00, -3.8933e+00,  4.4076e+00,\n        2.0517e+00, -6.6906e+00, -6.9448e+00,  6.0371e+00,  9.3081e-01,\n        1.5180e+00,  2.3974e+00, -3.8043e+00, -4.3941e+00, -3.6979e+00,\n        2.9489e+00, -8.9735e+00,  9.5273e+00, -6.4149e-01,  2.2565e+00,\n       -7.2062e+00, -1.0078e+00, -4.4381e+00,  2.0424e+00, -6.6736e-01,\n        4.3500e+00, -1.6199e+00,  3.1975e+00, -1.2065e+00, -6.5684e-01,\n        7.5759e-01, -1.6033e+00,  2.5450e+00, -5.4999e+00, -1.8909e+00,\n       -1.2985e-02,  2.6703e+00,  5.4623e-01, -2.4504e+00, -4.4326e-01,\n       -1.7250e+00,  9.1585e-01,  7.5243e+00, -5.8451e-01,  3.4550e+00,\n        3.4817e+00, -4.1599e+00, -5.5125e-01,  2.7681e-02, -3.1687e+00,\n       -4.8459e+00,  7.9108e+00, -1.7062e+00, -2.6731e+00,  9.7841e+00,\n        3.8851e+00, -3.7930e+00, -5.2979e-01,  6.6191e-01, -9.7232e-01,\n       -9.4692e-01, -4.4918e+00,  1.0932e+00, -4.3751e+00,  1.3182e-02,\n       -1.0243e+01,  4.7973e+00, -8.7426e+00,  2.5479e+00,  2.3454e+00,\n       -6.4140e+00,  7.3875e-01,  5.8565e+00, -2.5964e-01,  1.6558e+00,\n       -3.1353e+00, -6.6752e+00,  1.0550e+00,  1.7017e+00, -3.8360e+00,\n       -1.1980e+01, -1.3750e+00, -1.9261e+00,  3.1267e+00,  3.2874e+00,\n       -2.8928e+00, -1.0893e+01,  4.2848e+00, -4.0890e-02, -5.9565e-01,\n       -3.3473e-02,  1.6832e+00,  2.1454e-01,  7.2849e+00,  2.8116e+00,\n        2.5708e+00, -3.9823e-01, -1.7257e+00, -6.1063e+00, -4.2618e+00,\n       -3.3886e+00, -9.2663e+00,  1.7600e-01, -3.3873e-02, -3.7070e+00,\n       -9.1995e+00, -7.1594e+00, -6.0189e-01, -7.2560e-01,  1.5342e+00,\n        5.1083e+00,  2.4373e+00, -3.8012e+00, -2.1752e-01,  2.9503e+00,\n       -2.5551e+00,  4.9827e-01,  8.6823e-01, -4.3449e+00, -4.3821e+00,\n        3.4993e+00, -1.9518e+00,  2.2036e+00, -6.6526e-01,  7.1015e+00,\n        3.6784e+00,  2.6251e-01,  1.5379e+00, -8.1950e-01,  1.1065e+00,\n        3.3167e+00, -5.9392e+00, -4.0191e+00,  2.6496e+00,  2.3168e+00,\n       -8.5681e-02, -3.5059e+00,  1.5915e+00, -3.1831e-01,  6.9366e+00,\n        3.8439e+00,  9.4076e-01, -7.5424e+00,  2.7847e+00, -2.2814e+00,\n       -4.2487e+00, -2.6604e-01,  3.7954e+00, -3.6526e+00,  4.3823e+00,\n       -2.6506e+00,  3.5298e+00,  2.2597e+00,  6.3055e+00, -7.0194e-01,\n        4.1565e+00,  8.2306e+00,  5.7675e-01,  4.3596e-01, -8.8400e+00,\n       -3.0249e+00,  4.0032e+00,  2.4232e+00,  6.9885e+00, -2.5906e-01,\n       -4.2059e+00,  1.2643e+00,  1.0110e+01,  9.7016e-01,  2.2963e+00,\n       -1.2802e+00, -1.4447e+00, -3.4386e+00,  5.6555e+00,  3.3911e+00,\n        6.9418e+00, -6.8705e+00, -8.1536e-01, -7.2334e+00,  3.0509e+00,\n        8.7676e-01,  6.4216e+00, -3.1655e+00, -1.5308e+00, -1.1056e+00,\n       -5.0426e+00,  4.6801e+00,  4.6812e+00,  4.0401e+00, -3.7289e-01,\n        6.7437e-01, -8.6660e+00, -9.9656e+00,  2.4979e+00, -1.4783e-01,\n       -5.6301e+00,  4.5542e+00,  4.8165e+00, -2.2055e-01,  4.5169e+00,\n        1.7496e+00,  2.9019e-01, -1.1683e+00, -4.3981e-01,  2.3469e+00,\n       -4.3521e-02,  6.3715e-01,  5.8259e-01, -8.5701e+00,  4.6419e+00,\n        2.3809e+00, -1.9273e-01, -6.9772e+00,  7.6172e-01, -6.3895e-01,\n       -3.3769e+00,  6.1265e+00, -1.9695e+00, -2.3404e+00,  6.6789e+00,\n       -3.5265e+00, -3.3883e+00,  6.1372e+00,  4.5550e+00,  6.0957e+00,\n       -2.2007e-01,  6.2087e-01,  2.5527e+00, -4.5590e+00, -2.8429e+00,\n        2.0645e+00, -1.6221e+00, -2.8171e+00, -2.9680e+00,  1.3651e+00,\n        3.6137e+00, -3.2096e-01, -1.9346e+00, -4.8738e+00,  2.5565e+00],\n      dtype=float32)"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['king'].vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'kings', 'princess', 'queen', 'the', 'and', 'that', 'where', 'she', 'they']\n"
     ]
    }
   ],
   "source": [
    "# Get thr words vector\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "\n",
    "# Now we find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "new_vector = king - man + woman\n",
    "computed_similarities = []\n",
    "\n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors and mixed-case words:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                similarity = cosine_similarity(new_vector, word.vector)\n",
    "                computed_similarities.append((word, similarity))\n",
    "\n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
